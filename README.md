# Make Data Count + LLaMA 3

æœ¬å°ˆæ¡ˆå°ˆç‚º Kaggle çš„ Make Data Count æ¯”è³½è¨­è¨ˆï¼Œä½¿ç”¨ LLaMA 3 8B Instruct ç‚ºæ ¸å¿ƒæ¨¡åž‹ï¼Œæ•´åˆ Self-Evolving Learning (SEL) å››å¤§æ¨¡çµ„é€²è¡Œæ–‡ç»è³‡æ–™å¼•ç”¨åˆ†é¡žã€‚

ðŸ“Œ é–‹ç™¼ç›®æ¨™ï¼š
- è¼¸å…¥æ ¼å¼ï¼šPDF / XML
- è¼¸å‡ºæ ¼å¼ï¼šsubmission.csv
- Leaderboard > 0.910 åˆ†æ•¸

ðŸ“ å°ˆæ¡ˆçµæ§‹ï¼š

```
make-data-count-llama/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 â† PDF, XML åŽŸå§‹æª”æ¡ˆ
â”‚   â”œâ”€â”€ context/             â† context_unit è¼¸å‡º
â”‚   â”œâ”€â”€ predictions/         â† åˆ†é¡žçµæžœ / ä¿®æ­£çµæžœ
â”‚   â”œâ”€â”€ errors/              â† éŒ¯èª¤æ¨£æœ¬èˆ‡ç²¾ç…‰è¨˜éŒ„
â”‚   â”œâ”€â”€ cit_pairs/           â† CIT è¨“ç·´è³‡æ–™ï¼ˆprompt pairsï¼‰
â”‚   â”œâ”€â”€ rat_memory/          â† å‘é‡ç´¢å¼•èˆ‡èªžå¢ƒè³‡æ–™
â”‚   â””â”€â”€ submission/          â† CSV è¼¸å‡ºèˆ‡é©—è­‰å ±å‘Š
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ llama-3-8b-instruct/ â† ä¸»æ¨¡åž‹èˆ‡ tokenizer
â”‚   â””â”€â”€ lora_adapters/       â† LoRA è¨“ç·´å„²å­˜
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ parser/              â† L1
â”‚   â”œâ”€â”€ context_builder/     â† L2
â”‚   â”œâ”€â”€ llm_inference/       â† L3
â”‚   â”œâ”€â”€ refinement/          â† L4
â”‚   â”œâ”€â”€ meta_cognition/      â† L5
â”‚   â”œâ”€â”€ retriever/           â† L6
â”‚   â””â”€â”€ submission_writer/   â† L7
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ 01~07_xxx_test.ipynb â† å„å±¤ç´šæ¨¡çµ„æ¸¬è©¦èˆ‡å±•ç¤º
â””â”€â”€ README.md
```

```mermaid
flowchart TD
    subgraph L1 [L1 æª”æ¡ˆè§£æž]
        FP[FileParser]
        PX[PDFExtractor]
        XX[XMLExtractor]
        DR[DOIRecognizer]
        AM[AccessionMatcher]
    end
    subgraph L2 [L2 Context å»ºæ§‹]
        CB[ContextBuilder]
        SW[SlidingWindowContext]
        TAM[TitleAbstractMerger]
    end
    subgraph L3 [L3 æŽ¨ç†èˆ‡åˆ†é¡ž]
        LC[LLMClassifier]
        LI[LLaMA3Inference]
        LD[LLMOutputDecoder]
        PT[PromptPerturbationTester]
    end
    subgraph L4 [L4 è‡ªæˆ‘ç²¾ç…‰]
        RE[RefinementEngine]
        SQ[SelfQuestioner]
        SC[SelfCorrector]
    end
    subgraph L5 [L5 å¾Œè¨­å­¸ç¿’]
        EL[ErrorLogger]
        PG[PromptPairGenerator]
        LT[LoRAFineTuner]
    end
    subgraph L6 [L6 å‘é‡è¨˜æ†¶]
        CR[ContextRetriever]
        KR[KNNRanker]
    end
    subgraph L7 [L7 è¼¸å‡ºèˆ‡é©—è­‰]
        SG[SubmissionGenerator]
        KW[KaggleWriter]
        CSV[CSVSchemaValidator]
    end
    FP --> CB --> LC --> RE --> EL --> CR --> SG --> KW --> CSV
```

## Kaggle Notebook éƒ¨ç½²æŒ‡å—

1. å»ºç«‹ Kaggle Notebookï¼Œclone repo(ç¢ºèªrepoåˆ‡æ›åˆ°public)ã€‚
   ```bash
   !git clone https://github.com/Danwuoo/make-data-count-llama.git
   ```
2. å®‰è£ä¾è³´é …ã€‚
   ```bash
   !pip install -r /kaggle/working/make-data-count-llama/requirements.txt
   ```
   æ­¤æ™‚æœƒçœ‹åˆ°å ±éŒ¯ï¼Œè¼¸å…¥:
   ```bash
   !pip install "scipy>=1.10"
   ```
4. æŽ¥ä¸‹ä¾†æŠŠè¨“ç·´ç”¨çš„è³‡æ–™æŠ“åˆ°/data/raw/ã€‚
   ```python
   import shutil
   import os
   
   # åŽŸå§‹è³‡æ–™å¤¾
   pdf_dir = "/kaggle/input/make-data-count-finding-data-references/train/PDF"
   xml_dir = "/kaggle/input/make-data-count-finding-data-references/train/XML"
   
   # ç›®æ¨™è³‡æ–™å¤¾ï¼ˆçµ±ä¸€æ”¾åœ¨ raw ç›®éŒ„ä¸‹ï¼‰
   raw_dir = "/kaggle/working/make-data-count-llama/data/raw"
   os.makedirs(raw_dir, exist_ok=True)
   
   # è¤‡è£½ PDF
   for file in os.listdir(pdf_dir):
    src = os.path.join(pdf_dir, file)
    dst = os.path.join(raw_dir, file)
    shutil.copy(src, dst)
   
   # è¤‡è£½ XML
   for file in os.listdir(xml_dir):
    src = os.path.join(xml_dir, file)
    dst = os.path.join(raw_dir, file)
    shutil.copy(src, dst)
    ```
