{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "50bbcde7", "cell_type": "markdown", "source": "# Full Pipeline Test\nThis notebook demonstrates the end-to-end flow from parsing to submission.", "metadata": {}}, {"id": "549df205", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "from dataclasses import dataclass\nfrom utils.parsed_doc import ParsedDoc\nfrom utils.context_builder import build_context\nfrom utils.classifier import LLMClassifier\nfrom utils.submission_writer.kaggle_writer import KaggleWriter\n\n@dataclass\nclass DummyResult:\n    context_id: str\n    predicted_label: str\n    confidence: float\n    raw_output: str\n    prompt: str\n    logits: dict\n    meta: dict\n\n@dataclass\nclass DummyInference:\n    def infer(self, context_id: str, context: str, strategy: str = \"zero-shot\") -> DummyResult:\n        return DummyResult(context_id, \"primary\", 0.9, \"\", \"\", {\"primary\":1.0}, {})\n\ndoc = ParsedDoc(doc_id=\"DOC1\", source_type=\"pdf\", title=\"Sample\", abstract=\"Abstract\", body=\"Body text.\")\ncontexts = build_context(doc, max_tokens=20, stride=5)\nclassifier = LLMClassifier(inference=DummyInference())\npredictions = [classifier.classify(c.text, context_id=c.context_id) for c in contexts]\nwriter = KaggleWriter()\nwriter.write(predictions, \"data/submission/demo.csv\")", "outputs": []}]}